# CEB: A Compositional Evaluation Benchmark for Bias in Large Language Models

**[Paper Link]**: [CEB on Arxiv](https://arxiv.org/pdf/2407.02408)

Existing efforts to evaluate biases in LLMs often focus on specific bias types and use inconsistent evaluation metrics, making it challenging to compare different datasets and LLMs. 
To overcome the limitations, we introduce the Compositional Evaluation Benchmark (CEB) with 11,004 samples, based on a newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. Our experiments reveal varying levels of bias across these dimensions, providing valuable insights for developing targeted bias mitigation methods.

## Dataset

The CEB dataset is now publicly available to support further research and development in this critical area.

**[HugginFace Dataset Link]**: [CEB Dataset](https://huggingface.co/datasets/Song-SW/CEB)

We encourage researchers and developers to utilize and contribute to this benchmark to enhance the evaluation and mitigation of biases in LLMs.
